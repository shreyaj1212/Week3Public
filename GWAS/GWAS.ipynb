{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Problem 3: Genome Wide Association Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "(Adapted From BMI 704, Dr. Chirag Patel)\n",
    "\n",
    "Genome-wide association studies (GWASs) are arguably the most significant biomedical advance in the last decade. Utilizing an observational case-control design, populations with disease are analytically compared to those without to discern patterns in differences in genetic variant, or single nucleotide polymorphism (SNP),frequencies in each of the respective populations. If statistically differences in SNP frequencies in cases vs\n",
    "controls are found, one can conclude the SNP locus may have a causal relationship, implicating a gene, genetic region, or gene regulator in the disease of interest.\n",
    "\n",
    "Analyzing GWASs provide lessons in observational research, including statistical association and multiple testing. They also provide a way to create hypotheses about biological basis of disease for further study. We will execute a GWAS analysis using data from the Wellcome Trust Case-Control Consortium (WTCCC) on Type 2 Diabetes, Type 1 Diabetes, Bipolar Disorder, Rheumatoid Arthritis, Chron's Disease and Hypertension versus 1,500 controls.\n",
    "\n",
    "Note: There are standard tools and pipelines available to do this but we will do it the old fashioned way!\n",
    "\n",
    "The Data is provided in separate directories under the primary WTCCC directory. The subdirectories are labeled as such:\n",
    "* 58C : control population 1\n",
    "* NBS : control population 2\n",
    "* RA : rheumatoid arthritis\n",
    "* T2D : type 2 diabetes\n",
    "* T1D : type 1 diabetes\n",
    "* CAD : coronary artery disease\n",
    "* HT : hypertension\n",
    "* CD : Crohn’s disease\n",
    "* BD : bipolar disorder\n",
    "\n",
    "All of the GWAS data is broken down per chromosome, 1-22 plus X and is written into the file name, as well as the disease. For example:\n",
    "\n",
    "Affx_gt_ 58C _Chiamo_ 06 .tped.gz\n",
    "\n",
    "Contains GWAS data for the 6th (06) chromosome on control population 1 (58C). The biggest files are chromosome 1 and 2 and smallest is 22.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a SNP (there are 6207 SNPs measured in Chromosome 22). The first through fourth columns denote the chromosome number, the second is the WTCCC snp_id (not to be confused with the rsid), the start coordinate (0), and the end coordinate. Thereafter, each column is a genotype (where each base is separated by a space character) for that SNP for each individual.\n",
    "\n",
    "Additionally, each subdirectory also contains a ‘snps_info.tar.gz’, which contains the snp_id and rsID for each SNP for each chromosome. These files can enable you to map the WTCCC snp_id to the rsid, which is helpful to find the closest genes to a SNP (for example when you query the GWAS catalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "import csv\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import chisquare\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## CLASS NOTES ON WHAT TO DO ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the test data \n",
    "t2d_22_file = 'BWSI_100set/T2D/Affx_gt_T2D_Chiamo_22.tped.gz' ##########THE OG CODE SAID BWSI_test INSTEAD OF BWSI_100set\n",
    "t2d = pd.read_csv(t2d_22_file, compression='gzip', header=None, sep='\\t')\n",
    "c1 = pd.read_csv('BWSI_100set/58C/Affx_gt_58C_Chiamo_22.tped.gz', compression='gzip', header=None,sep='\\t')\n",
    "c2 = pd.read_csv('BWSI_100set/NBS/Affx_gt_NBS_Chiamo_22.tped.gz', compression='gzip', header=None,sep='\\t')\n",
    "snp = pd.read_csv('BWSI_100set/58C/snps/snps_22',header=None, sep='\\t')\n",
    "t2d_head = t2d.loc[:,0:3]\n",
    "t2d_snps = t2d.loc[:,4:]\n",
    "c1_head = c1.loc[:,0:3]\n",
    "c1_snps = c1.loc[:,4:]\n",
    "c2_head = c2.loc[:,0:3]\n",
    "c2_snps = c2.loc[:,4:]\n",
    "snp = snp.drop([0,1],axis=1)\n",
    "snp = snp.rename(columns={3:'snp_name',4:'rs'} )\n",
    "c1_head = c1_head.rename(columns={1:'snp_name'})\n",
    "snp=snp.drop([2],axis=1)\n",
    "snp_c1_joined = c1_head.join(snp.set_index('snp_name'), on='snp_name')\n",
    "t2d_1 = t2d_snps.loc[0,:]\n",
    "c1_1 = c1_snps.loc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(data):\n",
    "    arr = getAlleleCounts(data)\n",
    "    return(computeAlleleFrequency(arr[0],arr[1]))\n",
    "\n",
    "## compute Allele Frequency for the minor allele\n",
    "def computeAlleleFrequency (f_C, f_T):\n",
    "    \"\"\"\n",
    "    f_C = minor allele count\n",
    "    f_T = major allele count\n",
    "    minor_allele_frequency = f_C/ (f_C+f_T) \n",
    "    \"\"\"\n",
    "    minor_allele_frequency = f_C/(f_C+f_T)\n",
    "    return (minor_allele_frequency)\n",
    "\n",
    "def getAlleleCounts (genotype):\n",
    "    \"\"\"\n",
    "    genotype = \"C C \tC T \tC T \tC T \tT T\"\n",
    "    Allele frequency:\n",
    "    count the \"Cs\", and the \"Ts\"\n",
    "    f_C = # of Cs\n",
    "    f_T = # of T's\n",
    "    \"\"\"\n",
    "    return np.array([genotype.str.count('C').sum(),genotype.str.count('T').sum()])\n",
    "\n",
    "def getGenotypeCounts (genotype):\n",
    "    return np.array([genotype.str.count('CC').sum(),genotype.str.count('CT').sum(),genotype.str.count('TT').sum()])\n",
    "\n",
    "## Compute the Odds Ratio\n",
    "## takes as input a 2X2  confusion matrix\n",
    "## returns the odds ratio\n",
    "def computeOR(health,diabetic):\n",
    "    health = getAlleleCounts(health)\n",
    "    diabetic = getAlleleCounts(diabetic)\n",
    "    return diabetic[0]*health[1]/(health[0]*diabetic[1])\n",
    "\n",
    "## Execute a chisq test to determine if the odds ratio is significant\n",
    "## return the p-value from the chisq test\n",
    "def getPValue(confusionmatrix):\n",
    "    return (scipy.stats.chisquare(confusionmatrix).pvalue)\n",
    "\n",
    "def getPValue(confusionMatrix):\n",
    "    from scipy.stats import chisquare\n",
    "    return chisquare(confusionMatrix).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to load the data file\n",
    "def loadFile (disease, chrom_num):\n",
    "    ## construct your file name based on the disease and chrom_number\n",
    "    fileName = ''\n",
    "    dat = pd.read_csv(fileName, compression='gzip', header=None, sep='\\t')\n",
    "    return (dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds Ratio:\n",
      "0.9963249409766973\n",
      "\n",
      "Type 2 Diabetes:\n",
      "0.38244122061030517\n",
      "0.8141174907213498\n",
      "0.8154681593364395\n",
      "0.6175587793896948\n",
      "\n",
      "Control:\n",
      "0.38331117021276595\n",
      "0.616688829787234\n",
      "\n",
      "Chi-Squared:\n",
      "0.8141174907213498\n",
      "0.8154681593364395\n",
      "\n",
      "HWE:\n",
      "0.594335487291052\n",
      "0.9149630590292988\n"
     ]
    }
   ],
   "source": [
    "print(\"Odds Ratio:\")\n",
    "print(str(computeOR(c1_1,t2d_1)))\n",
    "\n",
    "print(\"\\nType 2 Diabetes:\")\n",
    "print(computeAlleleFrequency(getAlleleCounts(t2d_1)[0], getAlleleCounts(t2d_1)[1]))\n",
    "\n",
    "t1_minor = computeAlleleFrequency(getAlleleCounts(t2d_1)[0], getAlleleCounts(t2d_1)[1])\n",
    "t1_major = 1 - t1_minor\n",
    "control_minor = computeAlleleFrequency(getAlleleCounts(c1_1)[0], getAlleleCounts(c1_1)[1])\n",
    "control_major = 1-control_minor\n",
    "\n",
    "print(getPValue([t1_minor, t1_major]))\n",
    "print(getPValue([control_minor, control_major]))\n",
    "\n",
    "print(1-computeAlleleFrequency(getAlleleCounts(t2d_1)[0], getAlleleCounts(t2d_1)[1]))\n",
    "print(\"\\nControl:\")\n",
    "print(computeAlleleFrequency(getAlleleCounts(c1_1)[0], getAlleleCounts(c1_1)[1]))\n",
    "\n",
    "print(1-computeAlleleFrequency(getAlleleCounts(c1_1)[0], getAlleleCounts(c1_1)[1]))\n",
    "\n",
    "# print(control)\n",
    "\n",
    "print(\"\\nChi-Squared:\")\n",
    "print(getPValue([t1_minor, t1_major]))\n",
    "print(getPValue([control_minor, control_major]))\n",
    "\n",
    "print(\"\\nHWE:\")\n",
    "print(HWEChiSq(t1_minor, [t2d_1.to_string().count('C C'), t2d_1.to_string().count('C T'), t2d_1.to_string().count('T T')]))\n",
    "print(HWEChiSq(control_minor, [c1_1.to_string().count('C C'), c1_1.to_string().count('C T'), c1_1.to_string().count('T T')]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardy Weinberg Equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function takes as input the minor allele frequency (p) and the population genotype counts\n",
    "## returns the p-value from the chisq test\n",
    "def HWEChiSq (p, populationGenotypeCounts):\n",
    "    \"\"\"\n",
    "        |  CC | CT  | TT |\n",
    "    BP  | 270 | 957 | 771|\n",
    "    compute the HWE\n",
    "      -- p = frequency of the C allele\n",
    "      -- q = 1-p = frequency of the T allele\n",
    "      -- if the population genotype is in Hardy Weinberg Equilibrium, we would expect the genotype frequencies to be\n",
    "          CC = p^2*N \n",
    "          CT = 2pq*N \n",
    "          TT = q^2*N \n",
    "      -- to compute the deviation from HWE\n",
    "         (observed - expected)^2 / expected\n",
    "      -- do a chi.squared test to check if the deviation is significant\n",
    "    \"\"\"\n",
    "    total = sum(populationGenotypeCounts)\n",
    "        \n",
    "    q = 1-p\n",
    "    \n",
    "    observed = populationGenotypeCounts[0]\n",
    "    expected = p**2*total\n",
    "    \n",
    "    CC = (observed-expected)**2/expected\n",
    "    \n",
    "    \n",
    "    observed = populationGenotypeCounts[1]\n",
    "    expected = 2*p*q*total\n",
    "    \n",
    "    CT = (observed-expected)**2/expected\n",
    "    \n",
    "    observed = populationGenotypeCounts[2]\n",
    "    expected = q**2*total\n",
    "    \n",
    "    TT = (observed-expected)**2/expected\n",
    "    \n",
    "    from scipy.stats import chisquare\n",
    "    from scipy.stats.distributions import chi2\n",
    "    return chi2.sf(CC+CT+TT, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the data\n",
    "## manhattan plot\n",
    "## the X axis represents the chromosomes \n",
    "## the y-axis is the -log(p.value)\n",
    "def manhattanPlot (diseaseResults):\n",
    "    \"\"\"\n",
    "    group data by chromosomes, SNP before plotting\n",
    "    \"\"\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code to run the GWAS\n",
    "\n",
    "## Write a function that will take the following inputs:\n",
    "## (a) control files - 58C and NBS\n",
    "## (b) the disease - T2D, T1D, HT, BD, CD, RA\n",
    "## (c) the chromosome number\n",
    "\n",
    "## Your function should output a csv file with the following information\n",
    "## SNP Id, RSID, CHROMOSOME NUMBER, MINOR ALLELE, MAJOR ALLELE, MINOR ALLELE FREQUENCY, MAJOR ALLELE FREQUENCY, \n",
    "## ODDS RATIO (MINOR vs MAJOR ALLELE), P-Value for ODDS RATIO, Chi square for the ODDS RATIO, HWE deviation P-VALUE\n",
    "## \n",
    "\n",
    "def gwas (controlFile, disease, chrom_num):\n",
    "    \"\"\"\n",
    "    1. Load the control file for the specified chromosome number. Since there are two control populations, \n",
    "       it would be best to combine the two controls together\n",
    "    2. Load the disease file for the given chromosome number\n",
    "    3. Split the header from the SNP data for the controls and the disease\n",
    "    3. Load the SNP file -- this gives you the mapping between the WTCCC SNP ids and the RSID\n",
    "    4. For each SNP in the disease file and its matching SNP in the control file:\n",
    "       -- to make it easier, before you begin do a check for zygosity. If either the controls or the disease \n",
    "       are homozygous, skip the SNP\n",
    "       genotype CC, TT : skip\n",
    "       genotype CC CT TT : run the following analysis\n",
    "       a. get the minor and major allele counts and compute the minor allele frequency for the disease population\n",
    "       b. get the minor and major allele counts and compute the minor allele frequency for the control population\n",
    "       c. conduct the allelic test -- compute the odds ratios to test the frequency of the minor allele \n",
    "          in the disease population compared to the control population\n",
    "          -- you can build a confusion matrix based on the counts that you have calculated in a and b\n",
    "          -- compute your odds ratios based on the minor allele of the control population\n",
    "          \n",
    "          |                  | C    | T    | \n",
    "          |------------------+ ---  + ---  +\n",
    "          | Bipolar Disorder | 1529 | 2469 |\n",
    "          |------------------+----  + ---  +\n",
    "          | Healthy Controls | 2270 | 3738 |\n",
    "          |------------------+----- +----- +\n",
    "          \n",
    "          (Test : your OR for above should be :1.019)\n",
    "       d. Do a chi square test to test the significant and record the p-value\n",
    "       d. for each SNP calculate the deviation from Hardy-Weinberg equilibrium\n",
    "       e. create an output row containing:\n",
    "       SNP Id, RSID, CHROMOSOME NUMBER, MINOR ALLELE, MAJOR ALLELE, MINOR ALLELE FREQUENCY, \n",
    "       MAJOR ALLELE FREQUENCY, ODDS RATIO (MINOR vs MAJOR ALLELE), \n",
    "       P-Value for ODDS RATIO, CHI_SQ for the ODDS RATIO, HWE deviation P-VALUE  \n",
    "    5. Save the result file for the chromosome\n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write a main function that will invoke gwas for all chromosomes (1 through 22, don't include the X chromosome)\n",
    "\n",
    "def runGWAS (disease):\n",
    "    \"\"\"\n",
    "    1. For each chromosome 1:22 call the gwas function\n",
    "    2. Merge all the result files into one \n",
    "    3. Filter your data\n",
    "       a. remove all SNPs that deviate signficantly from HWE equilibrium -- these may indicate an error or \n",
    "          population specific deviation. For this remove all SNPs for which HWE deviation P-Value < 0.05\n",
    "       b. remove all SNPs that have minor allele frequency less than 1% -- not enough data\n",
    "    4. Draw a manhattan plot\n",
    "       c. draw a line at your Bonferroni threshold (Bonferroni correction): \n",
    "       if your significance is set at p.value < 0.05, then Bonferroni correction = 0.05/number of tests that you ran\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"manhattan.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many SNPs did you identify that exceeded the Bonferroni-level of significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much can you trust your results\n",
    "\n",
    "Typically, you want to examine the genome-wide distribution of your chi_square distribution against an expected null distribution. Visually you can do this using a quantile-quantile plot. Significant deviation from the null distribution may point to population stratification, familial relationships, technical bias, poor sample collection or sometimes even undetected sample duplications.\n",
    "(Slide courtesy - Dr. Chirag Patel)\n",
    "\n",
    "<img src=\"qqplot_example.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on your results do a quantile-quantile plot for your data\n",
    "\n",
    "##load your results \n",
    "##sort/order the -log10 of your p values in increasing order\n",
    "##generate an expected set of -log10(p_values)\n",
    "##plot the expected values on the x-axis and the observed -log10 (pvalues) on the y axis\n",
    "##Alternatively use: p = observed p values\n",
    "##stats.probplot(p, dist=\"norm\", plot=pylab)\n",
    "##pylab.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genomic Inflation Factor\n",
    "\n",
    "Mathematically, you can correct for some of the deviation by computing an inflation factor and then correcting for this inflation factor. This quantity is termed as genomic inflation factor ($\\lambda$)\n",
    "\n",
    "$\\lambda$ = observed median of test statistic distribution / expected median of the test statistic \n",
    "distribution.  For 1 degree of freedom, the X2 distribution has an expected median of 0.455\n",
    "\n",
    "$\\lambda$ <= 1.05 is considered acceptable. >1.1 is troubling, and indicates there is some inflation of the p values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the genomic inflation factor\n",
    "\n",
    "##labmda =  median(CHI2)/qchisq(0.5, 1)\n",
    "##if (gif > 1.05), correct your computed Chi.square values and recompute the p_values\n",
    "##chi2_corrected = CHI2/lambda, \n",
    "##pval_corrected = pchisq(chi2_corrected, 1, lower.tail = F)\n",
    "##re-draw the qqplot using the corrected pvalues (pval_corrected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this mean?\n",
    "\n",
    "Identify a SNP that is significant (above the Bonferroni threshold). \n",
    "What is the p-value of association and odds ratio for the SNP?\n",
    "What is the interpretation of the odds ratio? \n",
    "What gene is this SNP associated with and its putative function in your disease pathogenesis? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygenic Risk Scores (PRS)\n",
    "\n",
    "A PRS is the cumulative risk of all SNP for a single patient. For example, if a patient has a 2 snps with OR=2 each, then their PRS is 4. The underlying assumption here is that each allele has an equal contribution and there is no interation between SNPS, that is the SNPS are independent. \n",
    "\n",
    "$prs =\\sum_{i=1}^{m}-log_{10}(x_i) \\cdot n_i$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
